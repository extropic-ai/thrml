{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Restricted Boltzmann Machine\n",
    "\n",
    "This notebook demonstrates how to create, train, and sample from a Restricted Boltzmann Machine (RBM) using THRML.\n",
    "\n",
    "## What is an RBM?\n",
    "\n",
    "A Restricted Boltzmann Machine is an energy-based model with two layers:\n",
    "- **Visible layer**: Observed data\n",
    "- **Hidden layer**: Latent features\n",
    "\n",
    "The \"restricted\" aspect means there are no connections within a layer, only between layers (bipartite structure).\n",
    "\n",
    "The energy function is:\n",
    "\n",
    "$$E(v, h) = -\\beta \\left( \\sum_i a_i v_i + \\sum_j b_j h_j + \\sum_{i,j} W_{ij} v_i h_j \\right)$$\n",
    "\n",
    "where:\n",
    "- $v_i$ are visible units\n",
    "- $h_j$ are hidden units\n",
    "- $a_i, b_j$ are biases\n",
    "- $W_{ij}$ are connection weights\n",
    "- $\\beta$ is the inverse temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from thrml import SpinNode, Block, SamplingSchedule\n",
    "from thrml.models import (\n",
    "    RBMEBM,\n",
    "    RBMSamplingProgram,\n",
    "    RBMTrainingSpec,\n",
    "    rbm_init,\n",
    "    estimate_rbm_grad\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "key = jax.random.key(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Simple RBM\n",
    "\n",
    "We'll create a small RBM with 6 visible units and 3 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensions\n",
    "n_visible = 6\n",
    "n_hidden = 3\n",
    "\n",
    "# Create nodes\n",
    "visible_nodes = [SpinNode() for _ in range(n_visible)]\n",
    "hidden_nodes = [SpinNode() for _ in range(n_hidden)]\n",
    "\n",
    "# Initialize parameters with small random values\n",
    "key, subkey = jax.random.split(key)\n",
    "visible_biases = jax.random.normal(subkey, (n_visible,)) * 0.01\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "hidden_biases = jax.random.normal(subkey, (n_hidden,)) * 0.01\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "weights = jax.random.normal(subkey, (n_visible, n_hidden)) * 0.01\n",
    "\n",
    "beta = jnp.array(1.0)\n",
    "\n",
    "# Create the RBM\n",
    "rbm = RBMEBM(\n",
    "    visible_nodes=visible_nodes,\n",
    "    hidden_nodes=hidden_nodes,\n",
    "    visible_biases=visible_biases,\n",
    "    hidden_biases=hidden_biases,\n",
    "    weights=weights,\n",
    "    beta=beta\n",
    ")\n",
    "\n",
    "print(f\"Created RBM with {n_visible} visible units and {n_hidden} hidden units\")\n",
    "print(f\"Weight matrix shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the RBM\n",
    "\n",
    "We can sample from the joint distribution over visible and hidden units using block Gibbs sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thrml.block_sampling import sample_states\n",
    "\n",
    "# Create a sampling program that samples both visible and hidden units\n",
    "program = RBMSamplingProgram(\n",
    "    ebm=rbm,\n",
    "    free_blocks=[Block(visible_nodes), Block(hidden_nodes)],\n",
    "    clamped_blocks=[]\n",
    ")\n",
    "\n",
    "# Define sampling schedule\n",
    "schedule = SamplingSchedule(\n",
    "    n_warmup=100,      # Burn-in samples\n",
    "    n_samples=500,     # Number of samples to collect\n",
    "    steps_per_sample=2 # Gibbs steps between samples\n",
    ")\n",
    "\n",
    "# Initialize random state\n",
    "key, subkey = jax.random.split(key)\n",
    "init_state = rbm_init(\n",
    "    subkey, \n",
    "    rbm, \n",
    "    [Block(visible_nodes), Block(hidden_nodes)], \n",
    "    ()\n",
    ")\n",
    "\n",
    "# Sample from the model\n",
    "key, subkey = jax.random.split(key)\n",
    "samples = sample_states(\n",
    "    key=subkey,\n",
    "    program=program,\n",
    "    schedule=schedule,\n",
    "    init_state_free=init_state,\n",
    "    state_clamp=[],\n",
    "    nodes_to_sample=[Block(visible_nodes), Block(hidden_nodes)]\n",
    ")\n",
    "\n",
    "visible_samples, hidden_samples = samples\n",
    "\n",
    "print(f\"Visible samples shape: {visible_samples.shape}\")\n",
    "print(f\"Hidden samples shape: {hidden_samples.shape}\")\n",
    "print(f\"\\nVisible activation rate: {jnp.mean(visible_samples.astype(jnp.float32)):.3f}\")\n",
    "print(f\"Hidden activation rate: {jnp.mean(hidden_samples.astype(jnp.float32)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Sampling\n",
    "\n",
    "We can also sample hidden units given fixed visible units, which is useful for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sampling program with visible units clamped\n",
    "program_conditional = RBMSamplingProgram(\n",
    "    ebm=rbm,\n",
    "    free_blocks=[Block(hidden_nodes)],\n",
    "    clamped_blocks=[Block(visible_nodes)]\n",
    ")\n",
    "\n",
    "# Create some visible data\n",
    "visible_data = jnp.array([True, False, True, False, True, False], dtype=jnp.bool_)\n",
    "\n",
    "# Initialize hidden state\n",
    "key, subkey = jax.random.split(key)\n",
    "init_hidden = rbm_init(subkey, rbm, [Block(hidden_nodes)], ())\n",
    "\n",
    "# Sample hidden units given visible data\n",
    "schedule_short = SamplingSchedule(n_warmup=50, n_samples=100, steps_per_sample=1)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "hidden_samples_conditional = sample_states(\n",
    "    key=subkey,\n",
    "    program=program_conditional,\n",
    "    schedule=schedule_short,\n",
    "    init_state_free=init_hidden,\n",
    "    state_clamp=[visible_data],\n",
    "    nodes_to_sample=[Block(hidden_nodes)]\n",
    ")\n",
    "\n",
    "print(f\"Given visible pattern: {visible_data.astype(int)}\")\n",
    "print(f\"Sampled hidden states shape: {hidden_samples_conditional[0].shape}\")\n",
    "print(f\"Hidden activation probabilities: {jnp.mean(hidden_samples_conditional[0].astype(jnp.float32), axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Contrastive Divergence\n",
    "\n",
    "RBMs are typically trained using contrastive divergence, which estimates the gradient of the log-likelihood.\n",
    "\n",
    "The gradient update rules are:\n",
    "\n",
    "$$\\Delta W_{ij} = -\\beta (\\langle v_i h_j \\rangle_{data} - \\langle v_i h_j \\rangle_{model})$$\n",
    "$$\\Delta a_i = -\\beta (\\langle v_i \\rangle_{data} - \\langle v_i \\rangle_{model})$$\n",
    "$$\\Delta b_j = -\\beta (\\langle h_j \\rangle_{data} - \\langle h_j \\rangle_{model})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training specification\n",
    "schedule_positive = SamplingSchedule(n_warmup=10, n_samples=50, steps_per_sample=1)\n",
    "schedule_negative = SamplingSchedule(n_warmup=10, n_samples=50, steps_per_sample=1)\n",
    "\n",
    "training_spec = RBMTrainingSpec(\n",
    "    ebm=rbm,\n",
    "    schedule_positive=schedule_positive,\n",
    "    schedule_negative=schedule_negative\n",
    ")\n",
    "\n",
    "# Create a small training dataset\n",
    "batch_size = 8\n",
    "key, subkey = jax.random.split(key)\n",
    "training_data = [jax.random.bernoulli(subkey, 0.5, shape=(batch_size, n_visible)).astype(jnp.bool_)]\n",
    "\n",
    "# Initialize states for positive phase (hidden given visible)\n",
    "n_chains_pos = 2\n",
    "key, subkey = jax.random.split(key)\n",
    "init_hidden_pos = rbm_init(\n",
    "    subkey, \n",
    "    rbm, \n",
    "    [Block(hidden_nodes)], \n",
    "    (n_chains_pos, batch_size)\n",
    ")\n",
    "\n",
    "# Initialize states for negative phase (free sampling)\n",
    "n_chains_neg = 2\n",
    "key, subkey = jax.random.split(key)\n",
    "init_neg = rbm_init(\n",
    "    subkey, \n",
    "    rbm, \n",
    "    [Block(visible_nodes), Block(hidden_nodes)], \n",
    "    (n_chains_neg,)\n",
    ")\n",
    "\n",
    "# Estimate gradients\n",
    "key, subkey = jax.random.split(key)\n",
    "grad_weights, grad_visible_bias, grad_hidden_bias = estimate_rbm_grad(\n",
    "    key=subkey,\n",
    "    training_spec=training_spec,\n",
    "    visible_data=training_data,\n",
    "    init_state_positive=init_hidden_pos,\n",
    "    init_state_negative=init_neg\n",
    ")\n",
    "\n",
    "print(\"Gradient statistics:\")\n",
    "print(f\"Weight gradients - mean: {jnp.mean(grad_weights):.6f}, std: {jnp.std(grad_weights):.6f}\")\n",
    "print(f\"Visible bias gradients - mean: {jnp.mean(grad_visible_bias):.6f}, std: {jnp.std(grad_visible_bias):.6f}\")\n",
    "print(f\"Hidden bias gradients - mean: {jnp.mean(grad_hidden_bias):.6f}, std: {jnp.std(grad_hidden_bias):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Training Loop\n",
    "\n",
    "Let's implement a basic training loop with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_epochs = 5\n",
    "\n",
    "# Make a copy of the RBM for training\n",
    "trained_rbm = rbm\n",
    "\n",
    "print(\"Training RBM...\")\n",
    "print(f\"Learning rate: {learning_rate}, Epochs: {n_epochs}\\n\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Re-create training spec with updated parameters\n",
    "    training_spec = RBMTrainingSpec(\n",
    "        ebm=trained_rbm,\n",
    "        schedule_positive=schedule_positive,\n",
    "        schedule_negative=schedule_negative\n",
    "    )\n",
    "    \n",
    "    # Compute gradients\n",
    "    key, subkey = jax.random.split(key)\n",
    "    grad_w, grad_vb, grad_hb = estimate_rbm_grad(\n",
    "        key=subkey,\n",
    "        training_spec=training_spec,\n",
    "        visible_data=training_data,\n",
    "        init_state_positive=init_hidden_pos,\n",
    "        init_state_negative=init_neg\n",
    "    )\n",
    "    \n",
    "    # Gradient descent update\n",
    "    new_weights = trained_rbm.weights - learning_rate * grad_w\n",
    "    new_visible_biases = trained_rbm.visible_biases - learning_rate * grad_vb\n",
    "    new_hidden_biases = trained_rbm.hidden_biases - learning_rate * grad_hb\n",
    "    \n",
    "    # Create updated RBM\n",
    "    trained_rbm = eqx.tree_at(\n",
    "        lambda m: (m.weights, m.visible_biases, m.hidden_biases),\n",
    "        trained_rbm,\n",
    "        (new_weights, new_visible_biases, new_hidden_biases)\n",
    "    )\n",
    "    \n",
    "    # Compute approximate reconstruction error\n",
    "    grad_norm = jnp.sqrt(jnp.sum(grad_w**2) + jnp.sum(grad_vb**2) + jnp.sum(grad_hb**2))\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs} - Gradient norm: {grad_norm:.6f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Samples Before and After Training\n",
    "\n",
    "Let's see how the distribution changes after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from trained model\n",
    "program_trained = RBMSamplingProgram(\n",
    "    ebm=trained_rbm,\n",
    "    free_blocks=[Block(visible_nodes), Block(hidden_nodes)],\n",
    "    clamped_blocks=[]\n",
    ")\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "init_state_trained = rbm_init(\n",
    "    subkey, \n",
    "    trained_rbm, \n",
    "    [Block(visible_nodes), Block(hidden_nodes)], \n",
    "    ()\n",
    ")\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "samples_trained = sample_states(\n",
    "    key=subkey,\n",
    "    program=program_trained,\n",
    "    schedule=schedule,\n",
    "    init_state_free=init_state_trained,\n",
    "    state_clamp=[],\n",
    "    nodes_to_sample=[Block(visible_nodes)]\n",
    ")\n",
    "\n",
    "print(\"Visible unit statistics:\")\n",
    "print(f\"Original RBM: {jnp.mean(visible_samples.astype(jnp.float32), axis=0)}\")\n",
    "print(f\"Trained RBM:  {jnp.mean(samples_trained[0].astype(jnp.float32), axis=0)}\")\n",
    "print(f\"Training data: {jnp.mean(training_data[0].astype(jnp.float32), axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy Computation\n",
    "\n",
    "We can compute the energy of specific configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a specific configuration\n",
    "visible_state = jnp.array([True, True, False, False, True, False], dtype=jnp.bool_)\n",
    "hidden_state = jnp.array([True, False, True], dtype=jnp.bool_)\n",
    "\n",
    "# Compute energy\n",
    "state = [visible_state, hidden_state]\n",
    "blocks = [Block(visible_nodes), Block(hidden_nodes)]\n",
    "\n",
    "energy = trained_rbm.energy(state, blocks)\n",
    "\n",
    "print(f\"Visible configuration: {visible_state.astype(int)}\")\n",
    "print(f\"Hidden configuration:  {hidden_state.astype(int)}\")\n",
    "print(f\"Energy: {energy:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. Creating an RBM with specified dimensions\n",
    "2. Sampling from the joint distribution (visible and hidden)\n",
    "3. Conditional sampling (hidden given visible)\n",
    "4. Computing gradients via contrastive divergence\n",
    "5. Training the RBM with gradient descent\n",
    "6. Computing energies of configurations\n",
    "\n",
    "RBMs can be used for:\n",
    "- Dimensionality reduction\n",
    "- Feature learning\n",
    "- Collaborative filtering\n",
    "- Building blocks for deep belief networks\n",
    "\n",
    "For larger-scale applications (e.g., MNIST), you would:\n",
    "- Use larger dimensions (784 visible, 128-500 hidden)\n",
    "- Train with mini-batches\n",
    "- Use more sophisticated optimizers\n",
    "- Implement proper validation and early stopping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
